{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.4.7`\n",
    "\n",
    "import org.apache.log4j.{Level, Logger}\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "import org.apache.spark.sql._\n",
    "\n",
    "val spark = NotebookSparkSession\n",
    "    .builder()\n",
    "    .master(\"local[*]\")\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc = spark.sparkContext\n",
    "\n",
    "val rdd = sc.parallelize(1 to 100000000, 100)\n",
    "\n",
    "val n = rdd.map(x => x + 1).filter(x => x > 100).reduce((x, y) => x + y)\n",
    "\n",
    "// 0 1 2 3 4\n",
    "// map\n",
    "// 1 2 3 4 5\n",
    "// reduce(f)\n",
    "// f(1, 2) f(3, 4) 5\n",
    "// 3 7 5\n",
    "// f(3, 7) 5\n",
    "// 10 5\n",
    "// f(10, 5)\n",
    "// 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val lines = sc.textFile(\"data/crime.csv\")\n",
    "\n",
    "val lineLengths = lines.map(s => s.length)\n",
    "\n",
    "val totalLength = lineLengths.reduce((a, b) => a + b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val lines = sc.textFile(\"data/crime.csv\").cache()\n",
    "\n",
    "val totalLength = lines\n",
    "    .map(s => s.length)\n",
    "    .reduce((a, b) => a + b)\n",
    "\n",
    "val counts = lines\n",
    "    .flatMap(x => x.split(\",\"))\n",
    "    .map(s => (s, 1))     // RDD[(key, value)]\n",
    "    .reduceByKey((a, b) => a + b)\n",
    "    .sortBy(x => -x._2)\n",
    "    .take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "val crimeFacts = spark\n",
    "    .read\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"inferSchema\", \"true\")\n",
    "    .csv(\"data/crime.csv\")\n",
    "\n",
    "crimeFacts.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeFacts.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "$\"OFFENSE_CODE\" + 1\n",
    "\"OFFENSE_CODE\" + 1\n",
    "// crimeFacts.select($\"OFFENSE_CODE\" + 1)\n",
    "// crimeFacts.select(\"OFFENSE_CODE\" + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crimeFacts.createOrReplaceTempView(\"crimes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"select INCIDENT_NUMBER, DISTRICT from crimes limit 10\").show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "org.apache.spark.sql.catalyst.encoders.OuterScopes.addOuterScope(this) // Almond bug\n",
    "\n",
    "case class Crime (\n",
    "    INCIDENT_NUMBER: Option[String],\n",
    "    OFFENSE_CODE: Option[Int],\n",
    "    OFFENSE_CODE_GROUP: Option[String],\n",
    "    OFFENSE_DESCRIPTION: Option[String],\n",
    "    DISTRICT: Option[String],\n",
    "    REPORTING_AREA: Option[String],\n",
    "    SHOOTING: Option[String],\n",
    "    OCCURRED_ON_DATE: Option[String],\n",
    "    YEAR: Option[Int],\n",
    "    MONTH: Option[Int],\n",
    "    DAY_OF_WEEK: Option[String],\n",
    "    HOUR: Option[Int],\n",
    "    UCR_PART: Option[String],\n",
    "    STREET: Option[String],\n",
    "    Lat: Option[Double],\n",
    "    Long: Option[Double],\n",
    "    Location: Option[String]\n",
    ") {\n",
    "    \n",
    "    def wasShooting: Boolean = { SHOOTING.nonEmpty }\n",
    "    \n",
    "}\n",
    "\n",
    "crimeFacts.as[Crime].filter(x => x.OFFENSE_DESCRIPTION == Some(\"VANDALISM\")).show\n",
    "crimeFacts.as[Crime].filter(x => x.wasShooting).show\n",
    "crimeFacts.as[Crime].take(10).foreach(x => println(x.SHOOTING))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val offenseCodes = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"data/offense_codes.csv\")\n",
    "offenseCodes.show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "\n",
    "crimeFacts\n",
    "    .join(offenseCodes, $\"CODE\" === $\"OFFENSE_CODE\")\n",
    "    .where($\"NAME\".startsWith(\"ROBBERY\"))\n",
    "    .groupBy($\"NAME\")\n",
    "    .count()\n",
    "    .orderBy($\"count\".desc)\n",
    "    .show(false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions.broadcast\n",
    "\n",
    "val offenseCodesBroadcast = broadcast(offenseCodes)\n",
    "\n",
    "val robberyStatsWithBroadcast = crimeFacts\n",
    "    .join(offenseCodesBroadcast, $\"CODE\" === $\"OFFENSE_CODE\")\n",
    "    .filter($\"NAME\".startsWith(\"ROBBERY\"))\n",
    "    .groupBy($\"NAME\")\n",
    "    .count()\n",
    "    .orderBy($\"count\".desc)\n",
    "    \n",
    "robberyStatsWithBroadcast.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// udf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
